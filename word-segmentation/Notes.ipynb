{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分词是否必要？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "世界上这么多语言，如果直接从外观来看，大概有两种：一种是字符中间用空格隔开的，另一种是所有字符连在一起的。前者如英文、德语、俄语、法语等等，后者如中文、日文等，这类语言词与词之间没有用明显的标志分开。因此，分词就是想方设法将给定文本按一定方式隔开，便于计算机更好地处理。  \n",
    "\n",
    "我们自然而然就有一个疑问：为什么要分词，不分词可不可以？现有观点大概有这么几个：\n",
    "\n",
    "- “词” 是最小的能够 “独立运用” 的语言单位，尤其是现代汉语中双音节词占绝大多数，如果以字为基本单位不符合人的使用习惯。\n",
    "- 承载语义，单个字不太能表达准确的语义，比如 “手机”，单个字很难表达两个字所表达的意思。[为什么要分词 - CSDN 博客](http://blog.csdn.net/xzhunter/article/details/669401)也一语中的：“分词无非是切割语义罢了”。\n",
    "- 速度、准确度、成本、效率等\n",
    "  这个答案来自：[细说中文分词 |](https://www.biaodianfu.com/chinese-segmenter.html)，以搜索引擎为例，单个字组合的可能性太多，而且很多组合都是废的。比如：“蝴蝶”，任意一个字和其他所有汉字的组合都是没有意义的。\n",
    "- 切断上下文耦合，降低词序的影响。\n",
    "  这个答案来自：[自然语言处理为什么要分词？ - 毕胜的回答 - 知乎](https://www.zhihu.com/question/42022652/answer/220683270)，从自然语言处理任务的角度，认为大部分算法要求特征独立，如果不分词的话，字字之间几乎肯定有关联。所以就要想办法把文本切分一下，或者更好的说法叫组合一下，让各组合之间的耦合性降低，以组合为特征，使其更接近假设。\n",
    "\n",
    "总结一下，这么看来分词就是降低不确定性，也就是减熵，而减熵是智能的表现。因此，分词本质是把一定的 “先验知识” 加入到数据中，让数据具备一定的 “结构”。比如：“我|喜欢|你”，和 “我|喜|欢|你”，前者存储空间更小，信息量更小，看起来也更加合理。那这么说的话可不可以以句子为单位？当然也可以，但是句子生成的可能性是无穷的，复用性很差。更极端点的，全书没有一个标点看成一句也是可以的……但这样确实会影响数据的处理，尤其是要深入到具体字词级别的应用时，不过如果就是以整本书（或整个篇章）为分析对象，分不分词也问题不大，比如从统计学角度我们可以统计 “的” 字的数量，高频字、低频字，句子数量等等，这时候的感觉就像是一幅图画，每个字就是其中的一个像素点；当然按分词统计也没问题。  \n",
    "\n",
    "所以，要不要分词我们还需要看具体的任务，有的任务需要分词并且最好分词，比如搜索引擎、机器翻译、自动文摘、关键词抽取等等；也有的任务可以不分词，比如文本纠错、情感分析、文章分类、文本生成等，当然分词也是很 OK 的。正是因为无论从理论还是实际应用看都需要分词，所以我们平时见的 99% 的自然语言处理任务都是分词作为第一步的。  \n",
    "\n",
    "看来分词很有必要啦，但是且慢！我们上面的很多思路都是假定 “人” 作为处理者的，尤其是在自然语言处理任务中，而搜索其实是另一个问题（索引、查找、排序等），我们大多数时候会不由自主地将 “人” 代入 “计算机”，期望计算机也能像人一样思考和处理问题。事实是，就目前的计算机来说，那肯定和人完全不同的：计算机擅长存储、计算；人类擅长抽象、想象。计算机看到的图片是像素矩阵，人看到的图片是一种景象，一种真实世界在个人思维下的映射，计算机不知道美，即便知道也是人赋予的，是人告诉它 “这样像素的就是美”。反过来说，计算机为什么要像人一样呢？它本来就不需要像人一样。往大了说，人是一种物种，计算机也可以是一种物种，仅此而已，它就算理解你了，那也是 “看起来好像” 理解了。放到自然语言处理上说，目前通用的词向量就是把词表征成向量再喂入模型，那么字向量和词向量有什么区别？在计算机看来 “我喜欢你” 完全可以用四个字向量表示，虽然存储空间大了，但整个字表却比词表小得多，而且字表可以穷尽，词表却不可以。从这个角度看，我们有必要分词吗？  \n",
    "\n",
    "那么，我们到底要不要分词？我比较认可[为什么要分词 - CSDN 博客](http://blog.csdn.net/xzhunter/article/details/669401)这篇博客中的观点：“分词无非是切割语义”，仅此而已。试想想，可以不可以每两个字从头到尾切词切过去呢？换成四个字呢？可以不可以按读音的停顿切词呢？再想想，我们的书籍可以不可以切词呢？正如该博文作者所言：“中文没有分词功能，真的是很糟糕的，我一直认为中国之所以没有工业革命，就是因为没有分词。对于专业书籍来说，人进行分词也是非常困难的。至少分的很慢。所以影响了知识的传播。”这个观点你是否认同？到这里相信你应该对分词有了一些新的认识，而这对接下来如何分词有很重要的指导作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如何分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们假定按人类理解的角度分词，包括按词、按读音分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [漫话中文自动分词和语义识别（下）：句法结构和语义结构 | Matrix67: The Aha Moments](http://www.matrix67.com/blog/archives/4870)  \n",
    "- [趣题：这些词有什么共同点？ | Matrix67: The Aha Moments](http://www.matrix67.com/blog/archives/4858)\n",
    "- [关于北大中文系应用语言学（上）：更多有趣的汉语语法现象 | Matrix67: The Aha Moments](http://www.matrix67.com/blog/archives/508)\n",
    "- [另类思维训练：你能想出这样的词吗？ | Matrix67: The Aha Moments](http://www.matrix67.com/blog/archives/477)\n",
    "- [漫话中文自动分词和语义识别（上）：中文分词算法 | Matrix67: The Aha Moments](http://www.matrix67.com/blog/archives/4212)\n",
    "- [语言的对决：乔姆斯基攻防战 | 科学人 | 果壳网 科技有意思](https://www.guokr.com/article/156457/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [自然语言处理为什么要分词？ - 知乎](https://www.zhihu.com/question/42022652)\n",
    "\n",
    "- [Deep Learning for Chinese Word Segmentation and POS Tagging](http://www.aclweb.org/anthology/D/D13/D13-1061.pdf)\n",
    "- [深度学习将会变革 NLP 中的中文分词 | 雷锋网](https://www.leiphone.com/news/201608/IWvc75oJglAIsDvJ.html)\n",
    "- [【中文分词系列】 4. 基于双向 LSTM 的 seq2seq 字标注 - 科学空间 | Scientific Spaces](http://kexue.fm/archives/3924/)\n",
    "- [Second International Chinese Word Segmentation Bakeoff](http://sighan.cs.uchicago.edu/bakeoff2005/)\n",
    "- [互联网时代的社会语言学：基于 SNS 的文本数据挖掘 | Matrix67: The Aha Moments](http://www.matrix67.com/blog/archives/5044)\n",
    "- [有哪些比较好的中文分词方案？ - 知乎](https://www.zhihu.com/question/19578687)\n",
    "- [Tensorflow-Tutorial/Tutorial_6 - Bi-directional LSTM for sequence labeling (Chinese segmentation).ipynb at master · yongyehuang/Tensorflow-Tutorial](https://github.com/yongyehuang/Tensorflow-Tutorial/blob/master/Tutorial_6%20-%20Bi-directional%20LSTM%20for%20sequence%20labeling%20(Chinese%20segmentation).ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
